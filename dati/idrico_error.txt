Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/23 12:12:09 INFO SparkContext: Running Spark version 2.2.1
18/02/23 12:12:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/23 12:12:10 INFO SparkContext: Submitted application: Perceptron
18/02/23 12:12:10 INFO SecurityManager: Changing view acls to: dimartino
18/02/23 12:12:10 INFO SecurityManager: Changing modify acls to: dimartino
18/02/23 12:12:10 INFO SecurityManager: Changing view acls groups to: 
18/02/23 12:12:10 INFO SecurityManager: Changing modify acls groups to: 
18/02/23 12:12:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dimartino); groups with view permissions: Set(); users  with modify permissions: Set(dimartino); groups with modify permissions: Set()
18/02/23 12:12:11 INFO Utils: Successfully started service 'sparkDriver' on port 45239.
18/02/23 12:12:11 INFO SparkEnv: Registering MapOutputTracker
18/02/23 12:12:11 INFO SparkEnv: Registering BlockManagerMaster
18/02/23 12:12:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/02/23 12:12:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/02/23 12:12:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1cc105d9-5c23-4a27-8a04-b5c04cabe2ae
18/02/23 12:12:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/02/23 12:12:11 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/23 12:12:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
18/02/23 12:12:11 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
18/02/23 12:12:11 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
18/02/23 12:12:11 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
18/02/23 12:12:11 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
18/02/23 12:12:11 INFO Utils: Successfully started service 'SparkUI' on port 4045.
18/02/23 12:12:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.107.77.12:4045
18/02/23 12:12:11 INFO SparkContext: Added JAR file:/home/dimartino/Documenti/mario/codice/tesienea/perceptron/target/scala-2.11/perceptron_2.11-0.1.0-SNAPSHOT.jar at spark://192.107.77.12:45239/jars/perceptron_2.11-0.1.0-SNAPSHOT.jar with timestamp 1519384331813
18/02/23 12:12:11 INFO Executor: Starting executor ID driver on host localhost
18/02/23 12:12:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37021.
18/02/23 12:12:11 INFO NettyBlockTransferService: Server created on 192.107.77.12:37021
18/02/23 12:12:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/02/23 12:12:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.107.77.12, 37021, None)
18/02/23 12:12:11 INFO BlockManagerMasterEndpoint: Registering block manager 192.107.77.12:37021 with 366.3 MB RAM, BlockManagerId(driver, 192.107.77.12, 37021, None)
18/02/23 12:12:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.107.77.12, 37021, None)
18/02/23 12:12:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.107.77.12, 37021, None)
18/02/23 12:12:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dimartino/Documenti/mario/codice/tesienea/dati/spark-warehouse/').
18/02/23 12:12:12 INFO SharedState: Warehouse path is 'file:/home/dimartino/Documenti/mario/codice/tesienea/dati/spark-warehouse/'.
18/02/23 12:12:13 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
18/02/23 12:12:13 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/02/23 12:12:16 INFO FileSourceStrategy: Pruning directories with: 
18/02/23 12:12:16 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0)) > 0)
18/02/23 12:12:16 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
18/02/23 12:12:16 INFO FileSourceScanExec: Pushed Filters: 
18/02/23 12:12:16 INFO CodeGenerator: Code generated in 264.131754 ms
18/02/23 12:12:17 INFO CodeGenerator: Code generated in 24.287754 ms
18/02/23 12:12:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 277.4 KB, free 366.0 MB)
18/02/23 12:12:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.5 KB, free 366.0 MB)
18/02/23 12:12:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.107.77.12:37021 (size: 23.5 KB, free: 366.3 MB)
18/02/23 12:12:17 INFO SparkContext: Created broadcast 0 from load at perceptron.scala:22
18/02/23 12:12:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
18/02/23 12:12:17 INFO SparkContext: Starting job: load at perceptron.scala:22
18/02/23 12:12:17 INFO DAGScheduler: Got job 0 (load at perceptron.scala:22) with 1 output partitions
18/02/23 12:12:17 INFO DAGScheduler: Final stage: ResultStage 0 (load at perceptron.scala:22)
18/02/23 12:12:17 INFO DAGScheduler: Parents of final stage: List()
18/02/23 12:12:17 INFO DAGScheduler: Missing parents: List()
18/02/23 12:12:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at perceptron.scala:22), which has no missing parents
18/02/23 12:12:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.2 KB, free 366.0 MB)
18/02/23 12:12:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.3 KB, free 366.0 MB)
18/02/23 12:12:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.107.77.12:37021 (size: 4.3 KB, free: 366.3 MB)
18/02/23 12:12:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
18/02/23 12:12:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at perceptron.scala:22) (first 15 tasks are for partitions Vector(0))
18/02/23 12:12:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/02/23 12:12:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5309 bytes)
18/02/23 12:12:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/02/23 12:12:18 INFO Executor: Fetching spark://192.107.77.12:45239/jars/perceptron_2.11-0.1.0-SNAPSHOT.jar with timestamp 1519384331813
18/02/23 12:12:18 INFO TransportClientFactory: Successfully created connection to /192.107.77.12:45239 after 31 ms (0 ms spent in bootstraps)
18/02/23 12:12:18 INFO Utils: Fetching spark://192.107.77.12:45239/jars/perceptron_2.11-0.1.0-SNAPSHOT.jar to /tmp/spark-95ce1556-11bf-490f-8a4a-a0fd178ad58c/userFiles-a9775f00-2521-41a7-bdb3-ae6202376767/fetchFileTemp5207362222191363804.tmp
18/02/23 12:12:18 INFO Executor: Adding file:/tmp/spark-95ce1556-11bf-490f-8a4a-a0fd178ad58c/userFiles-a9775f00-2521-41a7-bdb3-ae6202376767/perceptron_2.11-0.1.0-SNAPSHOT.jar to class loader
18/02/23 12:12:18 INFO FileScanRDD: Reading File path: file:///home/dimartino/Documenti/mario/codice/tesienea/submit/idrico.csv, range: 0-1098968, partition values: [empty row]
18/02/23 12:12:18 INFO CodeGenerator: Code generated in 20.125822 ms
18/02/23 12:12:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1275 bytes result sent to driver
18/02/23 12:12:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 325 ms on localhost (executor driver) (1/1)
18/02/23 12:12:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/02/23 12:12:18 INFO DAGScheduler: ResultStage 0 (load at perceptron.scala:22) finished in 0,344 s
18/02/23 12:12:18 INFO DAGScheduler: Job 0 finished: load at perceptron.scala:22, took 0,505934 s
18/02/23 12:12:18 INFO FileSourceStrategy: Pruning directories with: 
18/02/23 12:12:18 INFO FileSourceStrategy: Post-Scan Filters: 
18/02/23 12:12:18 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
18/02/23 12:12:18 INFO FileSourceScanExec: Pushed Filters: 
18/02/23 12:12:18 INFO CodeGenerator: Code generated in 20.743999 ms
18/02/23 12:12:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 277.4 KB, free 365.7 MB)
18/02/23 12:12:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.5 KB, free 365.7 MB)
18/02/23 12:12:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.107.77.12:37021 (size: 23.5 KB, free: 366.2 MB)
18/02/23 12:12:18 INFO SparkContext: Created broadcast 2 from load at perceptron.scala:22
18/02/23 12:12:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
18/02/23 12:12:18 INFO SparkContext: Starting job: load at perceptron.scala:22
18/02/23 12:12:18 INFO DAGScheduler: Got job 1 (load at perceptron.scala:22) with 1 output partitions
18/02/23 12:12:18 INFO DAGScheduler: Final stage: ResultStage 1 (load at perceptron.scala:22)
18/02/23 12:12:18 INFO DAGScheduler: Parents of final stage: List()
18/02/23 12:12:18 INFO DAGScheduler: Missing parents: List()
18/02/23 12:12:18 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at load at perceptron.scala:22), which has no missing parents
18/02/23 12:12:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 14.1 KB, free 365.7 MB)
18/02/23 12:12:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.1 KB, free 365.7 MB)
18/02/23 12:12:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.107.77.12:37021 (size: 8.1 KB, free: 366.2 MB)
18/02/23 12:12:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
18/02/23 12:12:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at load at perceptron.scala:22) (first 15 tasks are for partitions Vector(0))
18/02/23 12:12:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/02/23 12:12:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5309 bytes)
18/02/23 12:12:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
18/02/23 12:12:18 INFO FileScanRDD: Reading File path: file:///home/dimartino/Documenti/mario/codice/tesienea/submit/idrico.csv, range: 0-1098968, partition values: [empty row]
18/02/23 12:12:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1567 bytes result sent to driver
18/02/23 12:12:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 224 ms on localhost (executor driver) (1/1)
18/02/23 12:12:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/02/23 12:12:18 INFO DAGScheduler: ResultStage 1 (load at perceptron.scala:22) finished in 0,225 s
18/02/23 12:12:18 INFO DAGScheduler: Job 1 finished: load at perceptron.scala:22, took 0,298974 s
Exception in thread "main" org.apache.spark.sql.AnalysisException: cannot resolve '`topic`' given input columns: [id, text.topic];;
'Project [id#12, text.topic#13, UDF('topic LIKE %true%) AS label#17]
+- Relation[id#12,text.topic#13] csv

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2884)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1150)
	at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:1905)
	at Perceptron$.main(perceptron.scala:25)
	at Perceptron.main(perceptron.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
18/02/23 12:12:18 INFO SparkContext: Invoking stop() from shutdown hook
18/02/23 12:12:18 INFO SparkUI: Stopped Spark web UI at http://192.107.77.12:4045
18/02/23 12:12:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/02/23 12:12:18 INFO MemoryStore: MemoryStore cleared
18/02/23 12:12:18 INFO BlockManager: BlockManager stopped
18/02/23 12:12:19 INFO BlockManagerMaster: BlockManagerMaster stopped
18/02/23 12:12:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/02/23 12:12:19 INFO SparkContext: Successfully stopped SparkContext
18/02/23 12:12:19 INFO ShutdownHookManager: Shutdown hook called
18/02/23 12:12:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-95ce1556-11bf-490f-8a4a-a0fd178ad58c
